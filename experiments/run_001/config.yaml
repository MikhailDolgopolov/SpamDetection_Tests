# experiments/run_001/config.yaml

# --------------------- DATASET ---------------------
datasets: ["enron", "lingspam"]   # list of dataset names you want to merge
test_size: 0.2                    # fraction of the data kept for *validation* inside the script (the split() function uses this)

# ------------------- VECTORIZER --------------------
vectorizer: ["tfidf"]               # either "count" or "tfidf"
analyzer: ["word", "char", "char_wb"]                # character‑wide binary bag‑of‑words
ngram_range: [[1, 1],[1, 2], [1,3], [2,3]]                # (min_n, max_n)
min_df: [3]                           # min document frequency (int or float)
max_df: [0.5]                         # max document frequency (float)

# --------------------- MODEL ----------------------
alpha: [0.1]                          # smoothing for MultinomialNB
cv_folds: 3                         # number of folds in GridSearchCV
scoring: "f1_macro"                 # metric to optimise

# --------------------- TRAINING --------------------
random_state: 42                    # reproducibility seed